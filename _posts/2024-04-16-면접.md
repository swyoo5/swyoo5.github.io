---
layout: single
title:  "4/16 기술면접 질문&답변"
categories: [Programming, 면접]
tag: [Programming, 면접]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

* sigomid보다 ReLU를 많이 쓰는 이유

  1. 계산량이 적다.

     sigmoid함수는 분모에 지수함수($e^{-x}$)가 있고 ReLU함수는 서로다른 선형함수(y = 0, y = x)가 결합된 비선형함수로 이루어져있어 **계산량이 ReLU함수가 더 적다.**

  2. 기울기소실 문제

     sigmoid함수는 x의 절댓값이 커지면(매우 큰 양수 또는 매우 작은 음수) 기울기가 0에 수렴하므로 역전파시 더이상 학습이 되지 않는 기울기소실문제가 발생한다. ReLU함수는 x가 음수인 경우에 마찬가지로 기울기가 0이 되므로 이를 해결하기 위해 ReLU함수에서 파생된 Leaky ReLU, parametric ReLU, exponential ReLU함수를 사용한다.

     

* Non linearity라는 말의 의미와 그 필요성은?

  비선형성 이라는 의미로 선형적이지 않다는 의미이다. 선형성은 x가 증가하거나 감소하면 **x의 변화량에 비례해서 y도 변화**한다는 의미이다. 즉 선형성은 예측가능하게 움직인다는 의미이며 비선형성은 예측가능하지 않게 움직인다는 의미이다. 비선형성의 필요성은 모델을 학습할 때 일상생활에서 예측가능하지 않은 복잡한 패턴을 학습할 때 활성화함수로 비선형함수를 사용한다. layer 3개에 대해서 활성화함수를 y = ax(a : constant)와 같은 선형함수를 사용한다고 하면 y = a(a(ax))가 되며 **$a^{3}$는 또다른 상수로 치환**이 가능하게 되므로 **layer를 여러개 쌓은 의미가 없어진다.**

  

* ReLU의 문제점은?

  sigmoid보다 ReLU를 많이 쓰는 이유는? 질문 참고

  

* ReLU로 어떻게 곡선 함수를 근사하나?

  하나의 은닉층을 가지는 인공신경망은 임의의 연속인 다변수 함수를 원하는 정도의 정확도로 근사할 수 있음을 말하는 **보편근사정리**에 의해서 ReLU함수는 곡선함수를 근사할 수 있다.



* gradient descent에 대해서 쉽게 설명한다면?

  목적함수에 대해서 global minimum을 찾는 최적화 기법으로 임의의 지점에서 시작해서 그 지점의 기울기가 음수라면 왼쪽, 양수라면 오른쪽으로 이동하며 최적해를 찾는 기법이다.



* 왜 꼭 gradient를 써야할까?

  global minimum은 y값이 작아지는 방향으로 이동해야 하기 때문에 기울기를 기반으로 방향을 찾는다.



* 그 그래프에서 가로축과 세로축은 각각 무엇인가?

  가로축은 파라미터, 세로축은 loss를 의미한다.

  

* 실제 상황에서는 그 그래프가 어떻게 그려질까?

  local minimum이 매우 많거나 saddle point가 생기는 고차원 함수로 그려질 가능성이 높다.

  

* GD중에 때떄로 loss가 증가하는 이유는?

  x값을 이동하는 폭을 결정하는 학습률이 너무 작으면 local minimum에 도달할 확률이 높고 학습률이 너무 높으면 최적해에 도달하지 못하고 계속 발산하게 된다.

  

* back propagation에 대해서 쉽게 설명한다면?

  신경망의 마지막에 계산된 loss를 다시 거꾸로 전달해서 그 loss를 기반으로 loss값이 작아지는 방향으로 은닉층의 가중치 값들을 수정해서 최적화하는 방법이다.

* local minima 문제에도 불구하고 딥러닝이 잘되는 이유는?

  실제 task에서는 변수의 개수가 많아져 목적함수의 차원이 매우 높아진다. 이러한 고차원 목적함수가 local minimum에 빠지기 위해서는 모든 변수가 local minimum에 빠져야 하므로 큰 문제가 되지 않는다.

  

* GD가 local minima문제를 피하는 방법은?

  문제 상황에 맞는 optimizer를 사용한다.

![img](https://blog.kakaocdn.net/dn/O7GFc/btrwZZP2m3P/69jm0qDmgLjj4CEKvjHDh0/img.png)

![img](https://blog.kakaocdn.net/dn/bRM6M8/btrwWhcxPgW/Z9ozfwbKKrmRpP5wuBuxv1/img.jpg)