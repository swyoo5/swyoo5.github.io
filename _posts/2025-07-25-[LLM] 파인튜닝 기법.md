```
layout: single
title:  "[LLM] 해부학 PDF 질의응답 RAG W17-20"
categories: [Programming, LLM, RAG, Project]
tag: [Programming, LLM, RAG, Project]
toc: true
author_profile: false
sidebar:
    nav: "docs"
```

# 텍스트 기법

| **No** | **텍스트** | **설명**                                                     |
| :----- | :--------- | :----------------------------------------------------------- |
| 1      | AdaLoRA    | SVD 개념 사용가중치 중요도에 따라 파라미터 자원 다르게 할당  |
| 2      | DyLoRA     | LoRA와 다르게 랭크 크기를 동적으로 바꿀 수 있음              |
| 3      | GaLore     | 가중치를 압축압축된 정보를 optimizer에 저장가중치 업데이트 시 다시 복원 |

# 멀티모달 기법

| **No** | **멀티모달** | **설명**                                                     |
| :----- | :----------- | :----------------------------------------------------------- |
| 1      | MM-LoRA      | 비전용, 텍스트용 병렬 LoRA 구성                              |
| 2      | QLadder      | CLIP 인코더로 추출한 특징과 QLadder에서 얻은 시각적 특징 결합이미지의 계층적 이해 |

# MoE 기법(추후 적용)

| **No** | **MoE**     | **설명**                                                     |
| :----- | :---------- | :----------------------------------------------------------- |
| 1      | MixLoRA     | 독립적인 attention-layer LoRA 활용 → 멀티태스크 학습에서 성능 향상각 태스크마다 최적의 전문가 선택(정적) |
| 2      | DynMoLE     | 하이브리드 라우팅 메커니즘을 통해 전문가 선택을 동적으로 조절라우터의 선택에 따름확신이 있는 경우 Top-K 전문가 선택(가장 확률 높은 K개)확신이 없는 경우 Top-P 전문가 고려(확률이 P 이상인 전문가) |
| 3      | HMoRA       | 계층적 MoE와 LoRA 결합예시 : 문장 전체의 의미와 단어 하나의 의미를 동시에 파악해야 하는 경우 → 각 단계의 전문가들이 협력 |
| 4      | TT-LoRA MoE | Tensor-Train 분해를 LoRA 가중치에 적용 → 네트워크 메모리 사용량 감소적은 메모리로 많은 전문가 운영 |
| 5      | X-LoRA      | 지식이 복합적으로 필요한 문제에서 전문가를 조합하여 더 정확하고 포괄적인 답변 제공 |

# 기법 종류

1. [도메인 특화 언어 모델 구축 방법](https://health-coding.tistory.com/107) 
   1. Base Model을 사용한 RAG보다 Fine Tuning을 결합했을 때 점수가 더 높았다.
2. [특정 도메인에 맞는 언어모델은 어떻게 만들까?](https://songys.github.io/2023Langcon/data/specific_domain.pdf)
   1. Data : 해당 도메인 데이터 많을수록 좋음. 4GB와 8GB, 12GB 사이에 유의미한 차이가 있지는 않다는 연구결과 있음. 1GB는 256K개의 기사가 필요
   2. Tokenizer : 
      1. 기존 tokenizer의 vocab을 그대로 쓴다
      2. Vocab을 새로 만든다
      3. 기존 tokenizer의 vocab에 새로운 단어 추가
   3. Difficulty of Task
      1. easy : base model 사용해도 무방
      2. difficult : Domain Specific LM 사용
3. [파인튜닝과 RAG로 완성하는 도메인 맞춤형 LLM 서비스 개발](https://fastcampus.co.kr/data_online_fine)
4. [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs’ Domain-specific Question-Answering Performance](https://arxiv.org/html/2505.15480v1)
   1. LLM의 기존 내제된 지식과 학습용 데이터 간의 충돌 발생
   2. 충돌이 크면 적은 가중치 적용(모델이 알고 있으면)
   3. 충돌이 작으면 큰 가중치 적용(모델이 모르면)
5. [BLADE: Enhancing Black-Box Large Language Models with Small Domain-Specifc Models](file:///C:/Users/유상원/Downloads/34620-Article Text-38687-1-2-20250410.pdf)
   1. 도메인 특화 소형모델 학습
   2. 소형모델의 생성 결과를 대형모델에 전달
   3. 지식과 답변 품질 모두 향상할 수 있음
6. [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://www.arxiv.org/abs/2505.17470)
   1. LLM이 파인튜닝 데이터셋의 질문들에 먼저 답변
   2. LLM이 자신의 답변을 평가해서 틀린 QA쌍을 걸러냄
   3. 틀린것들(모르는 지식)만 골라서 파인튜닝
7. [ChatEndoscopist: A Domain-Specific Chatbot with Images for Gastrointestinal Diseases](https://ebooks.iospress.nl/doi/10.3233/SHTI250478)
   1. 이미지 생성 챗봇 예시
   2. 페이지를 key로 검색된 청크의 페이지에 존재하는 이미지 가져옴
8. [QLoRA: Efficient Finetuning of Quantized LLMs](https://velog.io/@kaiba0514/QLoRA-QLoRA-Efficient-Finetuning-of-Quantized-LLMs)
   1. LoRA + Quantization
   2. LLaMA 65B 모델의 경우 사전학습할 때, 2,048대의 A100을 21동안 사용
   3. QLoRA 적용 → 단일 48GB GPU로 65B 모델 튜닝 가능
9. [LoftQ](https://velog.io/@nellcome/LoftQ에-대해-알아보자)
   1. LoRA + Quantization
   2. 양자화로 인해 잃은 정보를 보완하는 방법 제시
   3. original high-precision weight(W)와 Q + AB^t(양자화 weight + LoRA 어댑터 추가) 간의 차이를 구한 뒤, W를 Q + AB^t로 근사시킴
10. [AdaLoRA](https://dsbook.tistory.com/404)
    1. SVD 개념 사용
    2. 기존의 PEFT 방법은 가중치 행렬에 균등하게 자원을 분배 → 각 weight의 중요도를 고려하지 않음
    3. 가중치 행렬의 중요도에 따라 파라미터 자원을 다르게 할당한다는 아이디어
11. [DyLoRA](https://arxiv.org/abs/2210.07558)
    1. 기존 LoRA는 훈련 후에 랭크 크기를 수정할 수 없어서 처음부터 다시 훈련
    2. 최적의 랭크를 찾기 위해 exhaustive search가 필요
    3. DyLoRA는 단일 랭크가 아닌 여러 랭크 범위에서 LoRA 블록을 훈련 → 훈련 중에 어댑터 모듈이 정렬
    4. 하나의 모델로 다양한 랭크 지원 & 훈련 후 랭크 조정 가능
12. [DoRA](https://arxiv.org/abs/2402.09353)
    1. 벡터의 크기와 방향 개념 이용
    2. LoRA : 벡터의 크기와 방향을 동시에 조절 → 어떤 부분이 크기 변화인지, 방향 변화인지 구분 불가
    3. DoRA : 벡터의 크기와 방향을 따로 조절
    4. 전체 파인튜닝 : magnitude(각 뉴런의 중요도), direction(패턴 인식) 간의 음의 상관관계를 보임
       1. 특정 뉴런은 더 강하게 반응하지만, 방향은 덜 바뀜
       2. 특정 뉴런은 덜 강하게 반응하지만, 방향이 많이 바뀜
13. [GaLore](https://ostin.tistory.com/477)
    1. LLM에서 메모리를 가장 많이 먹는 부분은 optimizer 단계
    2. 각 파라미터마다 momentum과 variance를 저장해야 함 → 파라미터 개수의 3배 메모리가 필요
    3. 신경망의 gradient는 단순한 패턴을 가진다
       1. 4096x4096 크기의 행렬 → 1600만개의 숫자
       2. 실제로는 64x64개 정도의 에센셜이 있다
       3. 나머지는 핵심정보의 확장
    4. step
       1. 원본 gradient에서 중요한 패턴을 찾음(64x64)
       2. 4000개 숫자만 optimizer에 저장
       3. 실제 업데이트 할 때는 다시 1600만개로 복원
14. [임베딩 모델 파인튜닝](https://wikidocs.net/257342)
    1. 네거티브 샘플링
    2. 모델 융합

------

# 적용 가능

## 핵심 기법

1. DoRA

- ECT/KEPIC 문서의 전문 용어와 개념에 대한 미세조정
- LoRA대비 20-30% 향상된 성능으로 ECT 전문 지식을 더 정확하게 학습
- 방향과 크기 분리로 도메인 특화 가중치 최적화

1. QLoRA + 4비트 양자화

- 제한된 하드웨어에서 대용량 ECT 문서 학습
- 메모리 사용량 절약으로 더 큰 모델 학습 가능

## 멀티모달

[논문](https://www.themoonlight.io/ko/review/improving-multi-modal-large-language-model-through-boosting-vision-capabilities)에 따르면 MM-LoRA, QLadder의 조합이 시각적 인지 및 언어 이해 측면에서 효과적임

1. MM-LoRA(Multi-modal LoRA)

- 비전용, 텍스트용 두개의 병렬 LoRA로 구성
- ECT 문서의 텍스트와 이미지를 동시에 학습
- 회로도, 절차서의 이미지와 설명문을 연관 학습

1. QLadder

- CLIP 인코더로부터 추출한 특징 Fc와 QLadder에서 얻은 시각적 특징 결합
- ECT 이미지의 다층적 특징 추출
- 회로도의 세부 구성요소부터 전체 시스템까지 계층적 이해

## 최적화 기법

1. DPO(Direct Preference Optimization)

- ECT 전문가의 선호도 데이터로 직접 최적화
- 보상 모델 없이 도메인 전문성 향상
- 정확한 ECT 답변과 부정확한 답변 쌍으로 선호도 학습

1. KTO(Kahneman-Tversky Optimization)

- ECT 답변의 좋음/나쁨 평가로 학습
- 이진 피드백으로 평가 체계 구축

## 지속적 학습

1. O-LoRA

- 문서 업데이트, 규정 추가 시 활용
- 기존 지식 손실 없이 새로운 지식 추가
- 직교 부공간에서 새로운 태스크 학습으로 Catastrophic Forgetting 방지

1. LLaMA PRO

- 기존 모델 파라미터에서 기존의 블록을 얼리고 새로운 블럭을 추가
- 확장되는 블럭은 기존 블럭을 복사하고 linear layer의 일부만 0으로 초기화 
- ECT 특화 레이어 추가로 도메인 지식 주입
- 기본 언어 능력 유지하면서 도메인 전문성 강화

## 모델 병합 및 조합

1. MergeKit

- 여러 하위 도메인 전문 어댑터 통합
- TIES, SLERP 등 다양한 병합 방법으로 최적 조합 탐색

##  

------

# 검증 필요

## **1. 고급 PEFT 기법들 → 지금 task에는 불필요**

### **Mixture of Experts (MoE) 기반 LoRA**

- **MixLoRA**: LoRA 기반 MoE 방법으로 독립적인 attention-layer LoRA 어댑터를 활용하여 멀티태스크 학습에서 9% 정도 성능 향상 [Scale](https://scale.com/blog/fine-tuning-mixture-of-experts-peft)[GitHub](https://github.com/TUDB-Labs/MixLoRA)
- **DynMoLE**: Tsallis entropy를 활용한 하이브리드 라우팅 메커니즘으로 전문가 선택을 동적으로 조절
- **HMoRA**: 계층적 MoE와 LoRA 결합으로 토큰/태스크 레벨 라우팅을 통합 [Openreview](https://openreview.net/forum?id=IpLXuqmP0C)[Openreview](https://openreview.net/forum?id=lTkHiXeuDl)
- **TT-LoRA MoE**: tensor-train 분해를 통해 메모리 사용량을 30% 감소시키면서 전문가 네트워크 관리 [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts | AI Research Paper Details](https://www.aimodels.fyi/papers/arxiv/tt-lora-moe-unifying-parameter-efficient-fine)
- **X-LoRA**: 과학/기술 도메인에 특화된 동적 전문가 조합 시스템 [PEFT: Parameter-Efficient Fine-Tuning Methods for LLMs](https://huggingface.co/blog/samuellimabraz/peft-methods)

### **고급 LoRA 변형**

- **Möbius-inspired LoRA**: Möbius 기하학에서 영감을 받은 PEFT로 다중모달 생성 모델의 유연성과 표현력 향상 [Parameter Efficient Fine-Tuning for Multi-modal Generative Vision Models with Möbius-Inspired Transformation | International Journal of Computer Vision](https://link.springer.com/article/10.1007/s11263-025-02398-3)
- **Spectrum**: Signal-to-Noise Ratio 분석으로 가장 정보량이 많은 레이어만 선택적으로 파인튜닝 [How to fine-tune open LLMs in 2025 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2025)

## **2. 모델 병합 및 조합 기법**

### **LoRA 어댑터 병합**

- **MergeKit**: 다양한 병합 방법 지원 (TIES, SLERP, Task Arithmetic 등)으로 특화 어댑터들을 단일 모델로 통합 [Loraexchange](https://loraexchange.ai/guides/merging_adapters/)[Arcee](https://www.arcee.ai/blog/use-mergekit-to-extract-lora-adapters-from-any-fine-tuned-model)
- **LoRA-LEGO**: MSU(Minimal Semantic Units) 클러스터링을 통한 랭크별 모듈형 병합 [Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering](https://arxiv.org/html/2409.16167v1)
- **I-LoRA**: 라우팅 튜닝과 반복적 병합으로 지속적 학습 지원 [I-Lora: Iterative Merging of Routing-Tuned Low-Rank Adapters for Multi-task Learning | OpenReview](https://openreview.net/forum?id=CRkoMdDlFh)

### **Knowledge Distillation 활용**

- 대형 모델의 지식을 소형 특화 모델로 증류
- 다중 전문가 모델에서 단일 효율적 모델로 지식 압축

## **3. 고급 정렬 기법**

### **RLHF 대안들**

- **UNA (Unified Alignment)**: 다양한 정렬 방법을 통합하는 일반화된 암시적 보상 함수 [UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://arxiv.org/html/2410.21438v1)
- **ORPO**: 참조 모델 없는 단일 단계 선호도 정렬 [Post Fine Tuning LLM with Direct Preference Optimization](https://dida.do/blog/post-fine-tuning-llm-with-direct-preference-optimization)

### **향상된 PPO 구현**

- **GRPO**: PPO의 효율적 대안으로 별도 critic 모델 없이 메모리/연산 오버헤드 50% 감소 [GitHub](https://github.com/OpenRLHF/OpenRLHF)[Sebastianraschka](https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html)
- **REINFORCE++**: 더 간단하고 효율적인 정렬 접근법 [A vision researcher’s guide to some RL stuff: PPO & GRPO - Yuge (Jimmy) Shi](https://yugeten.github.io/posts/2025/01/ppogrpo/)

## **4. 해석가능성 및 제어**

### **Mechanistic Interpretability**

- **Activation Patching**: 모델 내부 작동 방식 이해를 위한 causal intervention [Apartresearch](https://apartresearch.com/sprints/apart-x-martian-mechanistic-router-interpretability-hackathon-2025-05-30-to-2025-06-01)[Neel Nanda](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
- **Circuit Tracing**: 모델의 내부 특징 간 상호작용을 추적하는 attribution graph [Stop guessing why your LLMs break: Anthropic’s new tool shows you exactly what goes wrong](https://dnyuz.com/2025/06/04/stop-guessing-why-your-llms-break-anthropics-new-tool-shows-you-exactly-what-goes-wrong/)
- **Sparse Autoencoders**: VLM의 내부 표현 분석 [Mechanistic Interpretability Meets Vision Language Models: Insights and Limitations | ICLR Blogposts 2025](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/)

### **라우팅 시스템**

- **Expert Orchestration**: 여러 특화 모델을 지능적으로 라우팅하는 시스템 [Apart x Martian Mechanistic Router Interpretability Hackathon | Apart Research](https://apartresearch.com/sprints/apart-x-martian-mechanistic-router-interpretability-hackathon-2025-05-30-to-2025-06-01)
- Judge 모델을 통한 능력 평가 및 최적 전문가 선택

## 5. Continual Learning

### Progressive Prompts

- 대부분의 파라미터를 고정하고 각 새로운 태스크에 대해 고정된 수의 토큰(프롬프트)만 학습

### DAPT

- 이중 주의 프레임워크로 LoRA 파라미터의 학습과 선택을 정렬

### Domain incremental CIT

- 도메인별 지시를 순차적으로 파인튜닝하여 새로운 도메인의 태스크 해결 능력 획득

## 6. Curriculum Learning

- 데이터를 쉬운 것부터 어려운 것 순서로 제시하여 모델이 점진적으로 학습하도록 하는 방법