---
layout: single
title:  "3/1 [논문해석] LLaMA "
categories: [Programming, LLaMA, NLP]
tag: [Programming, LLaMA, NLP]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# LLaMA : Open and Efficient Foundation Language Models

## Introduction

LLMs(Large Language Models)은 대규모 텍스트 말뭉치에서 학습되며 텍스트 지시 또는 몇개의 예시로부터 온 새로운 작업에 대해 모델의 성능을 보여준다. 이러한 **few-shot**(*n개의 예제를 주고 추론하려는 예시의 결과를 완성하도록 하는 접근법*) 속성은 모델을 충분한 크기로 스케일링 했을 때 나타난다. 이는 더 많은 파라미터의 개수가 더 나은 성능으로 이끌어 준다는 가정을 기반으로 한다.

하지만 **Hoffman et al.**에서 제한된 컴퓨팅 리소스에 대해 모델의 성능은 더 큰모델이 아닌 작은 모델로 더 많은 데이터를 학습할 때 최적의 성능을 보여준다고 말한다. 이 논문의 스케일링 법칙의 목적은 특정 훈련 컴퓨팅 리소스에 대해 "**어떻게 데이터와 모델의 크기를 가장 잘 스케일링 하는가**" 이다. 즉 훈련의 속도가 가장 빠른것이 아니라 유추(추론)하는데 가장 빠른 모델을 선호하고, 비록 대규모 모델을 특정 수준의 성능까지 훈련하는데 비용이 경제적이더라도 더 작은 모델을 더 오래 훈련시키는 것이 추론하는데 훨씬 경제적이다. 예를 들면 **Hoffman et al.**에서 2000억개의 토큰으로 1000억 모델(파라미터가 1000억개인 모델)을 훈련하는 것을 추천했더라도 70억 모델이 토큰이 1조개보다 늘어나도 성능이 계속 개선된다는 것이다.

## Approach

훈련 접근방법은 대규모 트랜스포머 모델에서 대규모 텍스트 데이터, 표준 Optimizer를 이용해 훈련했다.

### Pre-training data

훈련 데이터는 여러가지 소스로부터 가져와서 혼합시켰다. 오픈소스로 공개하기 위해 공적으로 이용가능하며 다른 LLM을 훈련하는데 영향력 있었던 데이터를 사용했다. 대괄호 안의 비율은 샘플링한 비율을 나타내며 아래는 사용한 데이터를 어떻게 전처리했는지에 관한 내용이다.

* English CommonCrawl[67%]
  * 2017~2020년 데이터 전처리
  * 줄 단위에서 중복제거
  * 영어가 아닌 페이지들을 제거, n-gram 언어모델로 품질이 낮은 내용을 필터링하기 위해 fastText 선형 분류기로 언어식별 수행
* C4[15%]
  * 중복제거, 언어식별(CommonCrawl 과 동일)
  * CCNet과 주요 차이점은 품질 필터링(구두점 존재여부, 웹페이지 단어, 문장 개수와 같은 경험적 방법에 의존한다.)
* Github[4.5%]
  * Apache, BSD, MIT 라이선스로 배포되는 프로젝트만 사용
  * 낮은 품질의 파일을 경험적 방법(줄 길이, 영어 숫자 비율)과 정규표현식을 사용하여 헤더와 같은 상용구를 제거
* Wikipedia[4.5%]
  * 20개의 문자(라틴, 키릴문자)를 커버
  * 하이퍼링크, 주석, 상용구 제거
* Gutenberg and Books3[4.5%]
  * 두개의 책 말뭉치를 사용
  * 내용이 90% 이상 겹치는 책은 제거
* ArXiv[2.5%]
  * 과학 관련 데이터를 추가
* Stack Exchange[2%]
  * 여러 영역(컴퓨터과학 ~ 화학)을 커버하는 고품질 QA 데이터
  * HTML 태그 제거
  * 점수 내림차순으로 정렬
* Tokenizer
  * Byte Pair Encoding(BPE) 알고리즘으로 토큰화
  * 모든 숫자를 각 자리수로 분리
  * 바이트를 UTF-8 문자로 분해

  전체적으로 훈련 데이터는 1조 4000억개의 토큰을 포함하며 대부분의 훈련데이터는 훈련중에 한번씩만 사용했다. 위키피디아, 책 데이터는 2에폭씩 사용했다.

### Architecture

논문의 모델(LLaMA)은 트랜스포머 기반의 모델이며 PaLM과 같이 다양한 모델로부터 이후 제안된 다양한 개선사항을 적용한다. LLaMA와 기존의 아키텍쳐와 주요 차이점은 다음과 같다([ ] 안의 모델에서 영감을 받음).

* 사전 정규화[GPT3]
  * 훈련의 안정성을 높이기 위해 출력을 정규화하는 대신 트랜스포머의 **서브층의 입력을 정규화**
  * **Zhang and Sennrich**에서 소개된 **RMSNorm** 정규화 함수 사용
* SwiGLU activation function[PaLM]
  * 활성화 함수를 ReLU에서 **SwiGLU** 함수로 대체
* Rotary Embeddings[GPTNeo]
  * 절대적 Positional Embedding 제거 => **Rotary Positional Embedding(RoPE)** 추가

LLaMA의 각기 다른 하이퍼파라미터는 표2에 나와있다.



![image-20240306173624636](/images/2024-03-01-LLaMA/image-20240306173624636.png)



### Optimizer

AdamW를 이용해 최적화. 하이퍼파라미터는 다음과 같다.
$$
\beta_{1}\ =\ 0.9 \\
\beta_{2}\ =\ 0.95
$$
코사인 학습률 스케줄을 적용해 마지막 학습률이 최대 학습률의 10%가 되도록 한다. 가중치 감쇠율(weight decay)은 0.1, gradient clipping은 1.0 사용한다. 2,000번의 웜업 스텝을 사용하고 모델의 크기에 따라 학습률, 배치의 크기를 다르게 한다.

### Efficient implementation

모델의 학습 속도를 향상시키기 위해 몇가지 최적화를 진행한다.

* 메모리 사용량, 런타임을 줄이기 위해 **멀티헤드 어텐션** 사용
* 훈련 효율성 향상을 위해 체크포인팅(gradient checkpointing)을 사용하여 역방향 전달중에 다시 계산되는 **활성화 함수를 줄임**(선형 레이어의 출력같이 계산량이 많은 활성화를 줄임, 모델과 시퀀스 병렬화를 통해 메모리 사용량을 줄임)
* 네트워크 상에서 활성화 연산, GPU간의 **통신을 최대한으로 중복**시킴

## Main results

zero-shot, few-shot에 대한 총 20개의 벤치마크(성능 평가에서 기준이 되어 정확도를 평가하는 데이터)에 대한 결과를 보고

* zero-shot : 텍스트로 된 설명과 테스트 예시를 제공한다. 모델은 개방형 생성으로 답변을 제공하거나 제안된 답변 순위를 메긴다.
* few-shot : 작업의 (1개~64개) 예시와 테스트 예시를 제공한다. 모델은 이를 입력으로 받아들이고 답변 생성하거나 다양한 옵션들의 순위를 메긴다.

연구진은 LLaMA를 자유형식 생성 작업, 다중 선택 작업에서 평가한다. 다중 선택 작업의 목표는 여러 선택지중에서 가장 적합한 결과를 선택하는 것이다. 연구진은 주어진 내용 중에서 가장 높은 가능도를 가진 결과를 선택한다. **Gao et al.**을 따라 결과에 있는 문자들의 갯수로 정규화한 가능도를 사용(OpenBookQA, BoolQ 제외)하며 "Answer:"가 주어진 결과의 가능도에 의해 정규화된 가능도를 기반으로 결과를 선택한다.

### Common Sense Reasoning

8가지의 흔한 벤치마크 데이터(BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA)를 사용한다. 이 데이터들은 **cloze 작업**(마스킹된 단어를 예측), **Winograd 작업**(문장의 문맥 이해, 예시로 문장에서 it이 가리키는것이 무엇인가), **다중선택 QA**를 포함한다.

LLaMA-65B는 Chinchilla-70B를 BoolQ를 제외하고(???LLaMA의 점수가 더 높은데???) 모든 벤치마크에서 더 나은 성능을 보였다. LLaMA-65B가 PaLM-540B를 BoolQ, winogrande를 제외하고 모든 벤치마크에서 나은 성능을 보였다. LLaMA-13B는 GPT-3보다 크기가 10배가 작지만 대부분의 벤치마크에서 더 나은 성능을 보였다.

![image-20240306185050282](/images/2024-03-01-LLaMA/image-20240306185050282.png)

### Closed-book Question Answering

LLaMA를 두개의 closed-book QA 벤치마크(Natural Questions, TriviaQA)에서 기존의 LLM과 비교한다. 두개의 벤치마크에서 LLaMA-65B가 zero-shot, few-shot 세팅에서 두 SOTA를 달성했다(표 4 : NatualQuestions, 표5 : TriviaQA). 그리고 LLaMA-13B가 GPT-3, Chinchilla보다 **5-10배정도 작지만 비슷한 성능**을 보였다.

![image-20240306185614295](/images/2024-03-01-LLaMA/image-20240306185614295.png)

![image-20240306185556789](/images/2024-03-01-LLaMA/image-20240306185556789.png)



### Reading Comprehension

RACE 독해 벤치마크에서 모델을 평가한다. 이는 중국 중.고등학생들의 영어독해 시험을 위해 설계된 데이터이다. 이 벤치마크에서 결과를 보면 LLaMA-65B는 PaLM-540B와 비슷했고, LLaMA-13B는 GPT-3보다 몇퍼센트 앞선것을 알 수 있었다.

![image-20240306231918673](/images/2024-03-01-LLaMA/image-20240306231918673.png)

### Mathmatical Reasoning

MATH와 GSM8k라는 두가지 수학적 추론 벤치마크에서 모델을 평가한다.

벤치마크에 대한 설명은 다음과 같다.

* MATH : LaTex로 쓰여진 중.고등학교 수학문제
* GSM8k : 중학교 수학문제 모음

표 7을 보면 PaLM과 Minerva를 비교한다. Minerva는 ArXiv와 Math Web Pages에서 추출된 38.5B개의 토큰으로 파인튜닝된 PaLM모델들의 시리즈(연속)이다. 반면 PaLM과 LLaMA는 수학적 데이터로 파인튜닝되지 않았다. PaLM, Minerva의 숫자는 **Lewkowycz et al.** 에서 가져온 것이며 maj1@k가 있는것과 없는것을 비교한다.

maj1@k는 각 문제에 대해서 k개의 샘플을 생성하고 다수 투표를 수행하는 평가를 의미한다. GSM8k에서 비록 LLaMA가 **파인튜닝되지 않았음에도** LLaMA-65B는 Minerva-62B를 능가했다.

### Code Generation

모델이 자연어 설명(지시)로부터 코드를 작성하는 능력을 평가한다. 벤치마크는 HumanEval, MBPP를 사용한다. 두가지 작업에서 모델은 몇개의 문장으로 이루어진 프로그램에 대한 설명을 전달받고 몇개의 입출력 예제를 전달받는다. 모델은 설명에 부합하고 테스트 케이스를 만족하는 파이썬 프로그램을 작성해야한다. 표 8에서 LLaMA와 코드에 파인튜닝되지 않은 PaLM, LaMDA와 pass@1 점수를 비교한다. PaLM와  LLaMA는 비슷한 수의 코드 토큰을 포함하는 데이터로 훈련한다.

표 8을 살펴보면 비슷한 수의 파라미터에 대해서 LLaMA는 코드에 특화되어 훈련되거나 파인튜닝되지 않은 LaMDA와 PaLM모델보다 성능이 뛰어나다. 구체적으로 LLaMA-13B는 LaMDA-137B보다 뛰어나고 LLaMDA-65B는 PaLM-62B보다 뛰어나고 심지어 PaLM의 훈련시간이 더 길었다. 이 표에 보고된 pass@1 결과는 temperature를 0.1로 샘플링하여 얻은 결과이다. pass@100과 pass@80은 temperature 0.8로 샘플링된 결과이다. 

코드 토큰에서의 파인튜닝으로 코드에서의 퍼포먼스 향상이  가능하다. 예를 들어 PaLM-Coder은 HumanEval 벤치마크에서 PaLM의 pass@1 점수를 26.2%에서 36%까지 증가시켰다.

![image-20240307000501819](/images/2024-03-01-LLaMA/image-20240307000501819.png)

[

참고로 temperature 파라미터는 샘플링할 때 무작위성을 얼마나 줄 것인지를 결정하는 파라미터이며 temperature이 낮을수록 결정론적인 답을, 높을수록 창의적인 답을 내놓는다.

]

### Massive Multitask Language Understanding(MMLU)

MMLU 벤치마크는 여러 영역(STEM, 인류, 사회과학 등)의 지식에 대한 다중선택 질문으로 구성되어있는 데이터이다. 벤치마크에 있는 예시를 사용하여 5-shot 세팅을 해야 모델을 평가했다. 이 벤치마크에서 LLaMA-65B는 Chinchilla-70B와 PaLM-540B에 비해 대부분의 영역에서 몇%정도 뒤쳐진다. 이에 대한 가능성있는 설명은 사전훈련 데이터에서 **제한된 양의 책과 논문**을 사용했다는 것이다. Arxiv와 Gutenberg and Books3의 용량은 177GB인 반면에 다른 모델들은 2TB의 용량의 책에서 학습했다.

### Evolution of performane during training

훈련동안 QA, Common Sense 벤치마크에서 모델의 성능을 추적했다(그림2). SIQA와 WinoGrande를 제외하고 대부분의 벤치마크에서 성능이 꾸준히 향상되었다. 주목할만한 것은, SIQA의 **데이터에 큰 변동(variance)**이 있었고 이는 데이터가 믿을만하지 않다고 판단할 수 있다. 그리고 WinoGrande에서 성능은 훈련 perplexity와 상관있지 않았다(LLaMA-33B와 65B가 유사한 성능).

## Instruction fine tuning

약간의 파인튜닝으로 MMLU에서 성능이 향상되었고 지시를 따르는 능력이 향상됨을 보인다. 표 10은 MMLU에서 LLaMA-1 모델과 기존의 지시 파인튜닝 모델(OPT-IML, Flan-PaLM)을 비교한다. LLaMA-1은 기존 파인튜닝 모델보다 성능이 뛰어났지만 SOTA와는 거리가 멀었다.

![image-20240307011251817](/images/2024-03-01-LLaMA/image-20240307011251817.png)

## Bias, Toxicity and misinformation

LLaMA는 다른 LLM과 마찬가지로 훈련데이터의 편향을 증폭시키고 유해한 내용을 생산해낼 수 있으므로 유해한 내용을 측정하고 편견을 감지하는 벤치마크로 평가를 한다.

### Real Toxicity Prompts

RealToxicityPrompts 벤치마크는 모델이 얼마나 유해한지를 평가하는 데이터이다. 이 데이터는 모델이 완료해야 하는 100k개의 프롬프트로 구성되어있으며 유해 점수를 매긴다. 점수는 0~1 사이의 실수이며 클수록 유해함을 나타낸다. **유해성은 모델이 커질수록 더 커진다**는것을 특히 Respectful 프롬프트에서 그랬다.

![image-20240307012100377](/images/2024-03-01-LLaMA/image-20240307012100377.png)

### CrowS-pairs

CrowS-pairs 데이터는 9가지 항목(성별, 종교, 인종, 성적 취향, 나이, 국적, 장애, 신체적 외모, 사회경제적 지위)에서 편견을 측정하는 데이터이다. 각 예시는 편견과 편견이 아닌것으로 구성되어있고 zero-shot 세팅에서 두 문장의 퍼플렉시티를 사용하여 편견이 있는 문장에 대해 모델의 선호를 측정한다. 점수가 높을수록 높은 편견을 의미한다. LLaMA는 종교, 나이, 성별순으로 제일 편향되었다.

![image-20240307013027183](/images/2024-03-01-LLaMA/image-20240307013027183.png)

### WinoGender

WinoGender은 모델의 참조성능이 대명사의 성별에 영향을 받는지를 평가하는 벤치마크이다. 각 문장에는 직업, 참가자, 대명사가 있다. 모델이 참조관계를 결정하도록 유도하고 문장의 맥락에 따라 올바르게 수행되었는지를 측정한다. 이 작업에서의 목표는 직업에 대한 사회적 편견이 모델에 의해 포착되었는지를 알아보는 것이다. her/her/she, his/him/he가 있을 때보다 their/them/someone이 있을 때 모델이 더 올바르게 잘 작동했다. 아마도 문장에서의 증거보다는 **직업을 이루는 대다수의 성별을 사용**하는 것으로 보인다. gotcha case(문제가 있는 케이스), 즉 문장의 대명사의 성별과 직업을 이루는 다수의 성별이 같지 않은경우, **LLaMA-65B가 더 많은 오류**를 일으켰다.

![image-20240307013147615](/images/2024-03-01-LLaMA/image-20240307013147615.png)

### TruthfulQA

TruthfulQA는 모델의 신뢰도, 즉, 주장이 사실인지를 식별하는 능력을 측정하는 벤치마크이다. 여기서 사실이란 실제세계에서 참인것을 참이라고 정의한다.

다시말해 잘못된 정보를 생성할 위험을 측정하는 과정이며 GPT-3보다 우수한 점수를 받았지만 올바른 정답의 비율은 여전히 낮았고 틀린 답변에 대해 **환각현상**을 일으킨다.

## Carbon footprint

모델을 학습시키는 과정에서 많은 에너지를 필요로 하는데 이에 따라 발생하는 이산화탄소 배출에 대한 책임이 있다.이 주제에 대한 **Wu et al.**에서 와트시(Wh)를 측정하는 공식을 따랐다.

* Wh : 모델을 훈련하는데 필요한 전력

* tCO2eq : 탄소 배출(단위 : 톤(t))

* PUE : 전력 사용 효율

Wh는 다음의 공식을 사용해 계산했다.

* Wh = GPU-h x (GPU 전력소비량) x PUE

탄소배출량은 센터의 위치에 따라 다르다. BLOOM은 0.057kgCO2eq/KWh만큼 방출하는 grid를 사용해 총 27tCO2eq를 배출하며 OPT는 0.231kgCO2eq/KWh만큼 방출해 총 82tCO2eq를 방출한다.

> 만약 같은 데이터센터에서 훈련했다면?

데이터센터의 위치는 고려하지 않고 US 국제 평균 탄소 집약도 계수(US national average carbon intensity factor)인 0.385kgCO2eq/KWh를 사용해 계산하면 공식은 다음과 같다.

* tCO2eq = MWh x 0.385

BLOOM과 OPT에 이 공식을 적용한다면 OPT는 80GB의 램을 가진 992개의 A100 GPU를 사용해 훈련하는데 34일 소요, 80GB 램을 가진 2048개의 A100 GPU를 사용해 모델 개발하는데 5달이 소요되는것으로 추정된다. 이는 모델을 개발하는데 2638MWh가 소요된다는 것을 의미하며 총 이산화탄소 배출량은 1015tCO2eq이다.

## Conclusion

* LLaMA-13B(크기가 10배 작음) > GPT-3
* LLaMA-65B ≈ Chinchilla-70B, PaLM-540B
* 기존의 연구와 다르게 공적으로 이용가능한 데이터로도 SOTA달성 가능



