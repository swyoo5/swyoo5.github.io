---
layout: single
title:  "1/20 "
categories: [Programming, python, cv2]
tag: [Programming, python, cv2]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# 프로젝트 주제 선정



# 데이터 설명



# 역할



# 와이어프레임



# 모델 선정 및 설명



# 학습결과

## 기존 모델 변경

문서에 포함된 모델에서 약간의 수정을 했다.

* EarlyStopping  추가
* 배치 크기 2로 수정
* b7 -> b0 모델로 변경

## 학습 결과

* 1.png

  ![1](/images/2025-01-20-팀플훈련결과/1.png)

  가장 처음 돌린 모델은, AI hub에 데이터를 올린 업체에서 만든 모델을 그대로 돌리려 했지만, 리소스의 이슈로 인해 코드 자체가 돌아가지 않았다. 그래서 배치 크기를 줄이고 모델을 efficientnet-b7에서 더 경량화된 모델인 b0(파라미터의 개수는 약 10배, 연산량은 약 100배 차이가 난다)모델을 돌렸다. AI hub 문서에서 정확도 89%였지만, 모델을 돌릴 수 있도록 수정했더니 80%로 떨어졌다.

* 2.png

  변경사항

  * hyper_param_batch = 2에서 4로 변경

  * 이미지 증강 코드 추가

    ```python
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                                           transforms.RandomResizedCrop(600, scale=(0.8, 1.0)),
                                            
    transforms.GaussianBlur(5, sigma=(0.1, 2.0))
    ```

  * 학습률 수정

    1e-4에서 1e-3으로 수정

  * 옵티마이저 변경

    Adam => AdamW로 수정

    Adam : Momentum + RMSProp

    AdamW : Adam 개선 버전으로, L2정규화를 올바르게 적용하도록 개선된 알고리즘

  * 스케줄러 변경

    StepLR => ReduceLROnPlateau로 수정

    성능 개선이 정체될 때 학습률을 줄여 과적합 방지

    ```python
    exp_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='max', factor=0.5, patience=5, verbose=True)
    ```

    파라미터 설명

    optimizer_ft : 옵티마이저 인스턴스, 학습률을 동적으로 조정한다.

    mode : 모니터링하는 지표가 최대인지, 최소인지 지정

    factor : 성능 향상이 없을 때, 학습률을 factor 배 만큼 줄인다.

    patience : 지정한 횟수동안 개선되지 않으면 학습률 감소

    verbose : 학습률이 감소할 때 메시지 출력

  * 이미지 해상도 조정

    (600, 600)에서 (800, 800)으로 수정