---
layout: single
title:  "10/19 딥러닝 활성화 함수, 손실 함수"
categories: [Programming, python, 자연어, AI, NLP, math]
tag: [Programming, python, 자연어, AI, NLP, math]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

#  Activation Function

활성화 함수는 인공 신경망의 뉴런에서 입력을 출력으로 변환하는 함수이다. 주로 비선형 함수롤 사용하며 신경망의 표현력을 향상시킨다. 활성화 함수를 사용하여 신경망은 비선형 관계를 모델링할 수 있으며, 다양한 복잡한 함수를 근사할 수 있다.

## Sigmoid Function(= Logistic Function)

* 활성화 함수 : 시그모이드 함수는 입력값을 (0, 1)로 압축한다. 딥러닝의 이진 분류 문제에서 출력 레이어의 활성화 함수로 주로 사용된다.

* 특징

  * 0과 1 사이의 출력을 생성하므로 확률로 해석할 수 있다.

  * 큰 입력값에 대해서 기울기 소실 문제가 발생할 수 있고, 학습이 느리거나 중단될 수 있다. 이러한 이유로 은닉층보다는 출력층에서 주로 쓰인다.

  * 시그모이드 함수는 이진 분류에 적합하며 다중 분류에는 softmax함수가 더 많이 사용된다.

    

## tanh Function

* 활성화 함수 : tanh 함수는 입력값을  (-1, 1)로 압축한다. 은닉층의 활성화 함수로 사용된다.
* 특징
  * 시그모이드 함수와 유사하지만 출력범위가 확장된다. 이로 인해 시그모이드 함수에 비해 기울기 소실 문제가 더 늦게 발생한다. 이러한 이유로 은닉층에서 많이 사용된다.
  * 음수 값에 대한 출력을 생성할 수 있으므로 중심이 0에 더 가깝게 있으므로 시그모이드 함수에서 갖고 있던 최적화 과정에서 느려지는 문제를 해결했다.
  * RNN과 LSTM과 같은 순환 신경망 구조에서 많이 사용된다.



## Step Function

* 임계값을 기준으로 출력을 이진 값으로 바꾼다.



## ReLU Function

* 입력이 양수이면 그 값을 그대로 출력하고, 음일 경우 0을 출력한다.
* 입력이 음수인 경우 기울기는 0이 되고, 이 뉴런은 죽게된다.(dying ReLU)
* 이러한 이유로 Leaky ReLU와 같은 변형된 ReLU(Modified ReLU)를 사용하기도 한다.



## Softmax Function

* 다중 클래스 분류 문제에서 출력층의 활성화 함수로 사용되며, 출력 값을 확률 분포로 변환한다.

# Loss Function

손실함수는 신경망의 출력값과 실제 값 사이의 차이를 측정하는 함수이다. 학습 중 이 손실함수를 최소화 하려고 노력하며, 모델의 성능을 평가하는 데 사용된다. 

## Cross Entropy Function

* 손실 함수 : 크로스 엔트로피 함수는 신경망의 출력과 실제 레이블간의 차이를 측정하여 모델을 학습하는데 사용한다.
* 특징
  * 이진 분류 문제에서 이진 크로스 엔트로피가, 다중 클래스 분류에서는 다중 크로스 엔트로피가 일반적으로 사용된다.
  * 크로스 엔트로피 손실은 모델의 예측값과 실제 레이블 간의 차이를 측정하여 손실을 최소화하도록 모델을 학습한다.

## Mean Squared Error

* 회귀 문제의 손실 함수로 사용되며, 예측값과 실제 값 간의 차이를 측정한다.



# Optimizer

옵티마이저는 손실 함수를 최소화하는 가중치와 편향 값을 찾는 알고리즘이다. 가중치 업데이트 방법을 결정하고, 각 옵티마이저는 가중치를 업데이트하는 방식과 속도를 조절하는 하이퍼파라미터를 다르게 가진다.

## Stochastic Gradient Descent

* 가장 기본적인 옵티마이저로, 모델의 가중치를 업데이트한다.
* Adam : 변화율과 제곱 변화율의 이동 평균을 사용하여 가중치를 업데이트한다.
* RMSprop : 각 파라미터의 학습률을 조정하여 가중치를 업데이트한다.



# Sigmoid & CrossEntropy

* 이진 분류 문제를 신경망으로 풀려면 점수에 Sigmoid Function을 적용해 확률로 변환하고, 손실을 구할 때는 손실 함수로 Cross Entropy Function을 사용한다. 
* 다중 분류의 경우, 출력층에서는 점수를 확률로 변환할 때 Softmax Function을, 손실 함수로는 Cross Entropy Function을 이용한다. 