---
layout: single
title:  "12/04 [논문리뷰] Recurrent neural network based language model"
categories: [Programming, python, 논문, LSTM]
tag: [Programming, python, 논문, LSTM]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

 이 포스팅은 논문 Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling(Hasim Sak 외 2인)을 해석하고 요약한 글입니다.



# Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling(Hasim Sak)

## Abstract

LSTM은 시간적 순서 그리고 일반적인 RNN보다 그들의 긴 범위의 의존도를 더 정확하게 모델링하도록 디자인된 특별한 RNN 구조이다. 이 논문에서 우리는 음성인식에서 큰 규모의 음향 모델링을 위한 LSTM RNN 구조를 알아볼 것이다. 우리는 최근에 LSTM RNN이 한 기계에서 훈련된 적당한 크기의 모델을 고려한 음향 모델링에 있어서 DNN, 일반적인 RNN보다는 더 효율적이라는 것을 보였다. 우리는 큰 군집의 머신에서 비동기식 SGD 최적화를 이용한 LSTM RNN의 최초의 분산 교육을 소개한다. 우리는 각 LSTM의 계층이 선형의 반복되는 투사층을 가진 2 계층의 LSTM RNN이 음성인식의 최신 모델의 성능을 넘을 수 있다는 것을 보여준다. 이 구조는 다른 모델이 고려한 것보다 모델의 파라미터의 사용을 더 효율적으로 사용하고 빠르게 수렴하며 더 많은 파라미터를 가진 깊은 피드포워드 신경망보다 더 나은 성능을 낸다.

Index Terms : Long Short Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling



# 1. Introduction

말은 다양한 시간 범위에서 복잡한 상관관계가 있는 시변동성 신호이다. RNN은 피드 포워드 신경망보다 시계열 데이터를 모델링하는 더 강력한 도구로 만들어주는 순환 연결을 포함한다. RNN은 손글씨 인식 그리고 언어 모델링과 같은 시퀀스 라벨링 그리고 예측 업무에서 큰 성공을 거두었다. 하지만 심층 신경망이 인정받는 최첨단 기술인 음싱인식을 위한 음향모델링에서 최근데 RNN이 소규모 전화인식 작업 외에는 관심을 받지 못했다. 주목할만한 예외는 Robinson, Graves, Sak의 연구이다.

  DNN은 음향 프레임의 고정된 크기의 sliding window를 작동함으로써 제한된 시간적 모델링만을 제공할 수 있다. DNN은 window 내에 있는 데이터만 모델링 할 수 있고 **다양한 말하기 속도**와 **장기 종속성**을 다루는데는 적합하지 않다. 대조적으로 RNN은 현재 시점의 예측에 영향을 주기 위해 이전 시점의 네트워크 활성화값을 네트워크의 입력으로 제공하는 cycle을 가지고 있다. 이러한 활성화 값들은 원칙적으로 장기간의 시간 문맥 정보를 포함하는 네트워크의 은닉값에 저장돼있다. 이러한 메카니즘은 RNN이 피드 포워드 네트워크에 사용되는 고정된 크기의 window와 같이 정적 window가 아닌 입력 시퀀스 기록에 대해 동적으로 변하는 contextual window를 사용할 수 있게 한다. 특히 RNN의 모델링 약점을 극복한 LSTM 구조는 개념적으로 음향 모델링 작업에 적합하다.

  LSTM과 일반적인 RNN은 성공적으로 다양한 시퀀스 예측과 시퀀스 라벨링 작업에 적용됐다. 언어 모델링에서 일반적인 RNN은 표준 n-gram 모델에서 커다란 perplexity의 감소를 얻어냈고 LSTM RNN 모델은 일반적인 RNN LM을 넘는 향상을 보여줬다. LSTM 모델은 RNN 모델보다 문맥에서 자유로운 언어 그리고 문맥에 의존하는 언어들을 배우는데 더 좋은 성능을 보여줬다.  현재 입력을 위한 결정을 내리기 위해 양방향에서의 입력 시퀀스에서 작동하는 양방향 LSTM (BLSTM) 네트워크는 TIMIT 음성 데이터베이스에서 음향 프레임의 음성 라벨링을 위해 제안되었다. 온라인 그리고 오프라인 손글씨 인식에서 Connectionist Temporal Classification(CTC) 층과 함께 사용되고 분할되지 않은 시퀀스 데이터로부터 훈련을 하는 BLSTM 네트워크는 최신의 Hidden Markov Model(HMM)보다 더 나은 성능을 보여줬다. deep BLSTM 네트워크와 비슷한 기술들이 **자소** 기반 음성인식을 수행하기 위해 제안되었다.

* 자소 : 한 언어의 문자 체계에서 음소를 표시하는 최소의 변별적 단위로서의 문자 혹은 문자 결합

BLSTM 네트워크는 연속적인 대화 음성인식을 위한 다중 스트림 프레임워크에서 음소 예측을 위해 제안되었다. 아키텍쳐의 관점에서 음향 모델링을 위한 DNN의 성공에 이어 deep BLSTM RNN은 CTC 출력층과 함께 결합하고 RNN 변환기 예측 음소 시퀀스는 TIMIT 데이터베이스에서 최첨단 모델의 음소 인식 정확도에 도달한 것으로 나타났다.

  Deep BLSTM RNN은 최근에 복합적인 음성 인식 접근법에서의 DNN보다 더 나은 성능을 보여준다. 복합적인 방식을 사용하여 우리는 단일 머신을 사용하여 훈련된 적당한 크기의 모델을 고려할 때 순환 투사층을 가진 LSTM 아키텍처가 DNN과 대규모 단어 음성 인식을 위한 일반적인 RNN보다 우수하다는 것을 최근에 보여주었다. 이 논문에서 분산 훈련을 사용해 대규모의 음향 모델링을 위한 LSTM RNN 구조에 대해 알아본다. 우리는 각 LSTM 계층이 선형 순환 투사층을 가진 2층 deep LSTM RNN이 더 많은 파라미터를 가진 피드 포워드 신경망을 사용한 강력한 기본 시스템보다 성능이 뛰어남을 보여준다.



## 2. LSTM Network Architectures

### 2.1 Conventional LSTM

LSTM은 순환 은닉층 안에 **memory block**이라고 불리는 특별한 유닛을 포함한다. **memory block**은 정보의 흐름을 통제하는 게이트라고 불리는 특별한 곱셈 단위 외에 네트워크의 시간적 상태를 저장하는 자체 연결이 있는 메모리 셀을 포함한다. 원래의 구조 안에 있는 각 **memory block**은 *input gate*와 *output gate*을 포함한다. *input gate*는 메모리 셀 안의 입력 활성화 값의 흐름을 통제한다. *output gate*는 나머지 네트워크 안의 셀 활성화 값의 출력 흐름을 통제한다.

![lstm 논문리뷰 그림](/images/2023-12-04-LSTM/lstm 논문리뷰 그림.jpg)

그림1: LSTM RNN 단일 메모리 블록이 표시되어있다.

나중에 forget gate가 메모리 블록에 추가된다. 이는 LSTM 모델의 약점을 해결했으며 모델을 연속적인 입력 스트림을 부분 시퀀스로 나누지 않고 처리하는 것을 방지한다. forget gate는 셀의 자체 순환 연결을 통해 셀의 입력으로 추가하기 전에 셀의 내부 상태를 조정한다. 따라서 셀 메모리를 적응적으로 잊어버리거나 재설정한다. 게다가 현대의 LSTM 아키텍쳐는 출력의 정확한 타이밍을 학습하기 위해 내부 셀에서 동일한 셀의 게이트까지 *peephole connections*(엿보기 구멍 : Gate layer들이 cell state를 쳐다보게 만드는 모델)을 포함한다.

  LSTM 네트워크는 t = 1부터 T까지 반복적으로 다음 방정식을 사용하여 네트워크 유닛 활성화를 계산하여 입력시퀀스 x = (x1, ... , xT)에서 출력 시퀀스 y = (y1, ... , yT)로의 매핑을 계산한다.

![lstm 논문리뷰 수식](/images/2023-12-04-LSTM/lstm 논문리뷰 수식.jpg)

W는 가중치 행렬, (Wix는 입력 게이트에서 입력 게이트로의 가중치 행렬) Wic, Wfc, Woc는 peephole connectino을 위한 대각 가중치 행렬, b는 편향 벡터, sigma는 로지스틱 시그모이드 함수, i, f, o, ㅊ는 각각 input, forget, ouput gate, cell activation vector을 의미하며 이 모든것들은 cell output activation vector m과 같은 크기를 가지고 동그란 모양의 연산은 원소별 곱, g, h는 cell input과 cell ouput 활성화 함수로 일반적으로 이 논문에서 tanh, 6번째 수식에 있는 함수는 softmax를 의미한다. 



### 2.2 Deep LSTM

더 깊은 아키텍처를 갖춘 DNN과 마찬가지로 deep LSTM RNN은 음성인식에서 성공적으로 사용된다. Deel LSTM RNN은 다중 LSTM 계층을 쌓음으로써 만들어진다. LSTM RNN은 각 층이 같은 파라미터를 공유하는 시점에 따라 펼쳐진 피드 포워드 신경망으로 간주될 수 있다는 관점에서 이미 심층 아키텍처이다. 모델의 입력이 DNN처럼 여러 비선형 계층을 통과하는 것을 알 수 있다. 그러나 특정 시점의 feature은 해당 시점의 출력에 영향을 미치기 전에 단일 비선형 계층에 의해서만 처리된다. 따라서 deep LSTM RNN의 깊이는 추가적인 의미를 갖는다. 주어진 시점의 네트워크의 입력은 시간을 통한 전파 그리고 lstm 계층 외에 여러 LSTM 계층을 지나간다. RNN의 깊은 계층을 사용하면 네트워크가 입력에 대해 여러 시간 규모에서 학습을 할 수 있다고 주장되어왔었다. Deep LSTM RNN은 표준 LSTM RNN에 비해 다른 이득을 제공한다 : 매개변수를 여러 계층을 통해 공간에 분산시켜 매개변수를 더 잘 사용할 수 있게 한다. 예를 들어 표준 모델의 메모리 크기를 2배로 늘리는 대신 대략 같은 수의 매개변수를 가진 4개의 계층을 가질 수 있다. 이로 인해 입력이 시점당 더 많은 선형 작업을 거치게 된다.



### 2.3 LSTMP- LSTM with Recurrent Projection Layer

표준 LSTM RNN 아키텍처는 입력층, recurrent LSTM층, 출력층을 가지고 있다. 입력층은 LSTM층과 연결되어있다. LSTM층의 순환 연결들은 셀 출력 유닛들에서 셀 입력 유닛, 입력 게이트, 출력 게이트 그리고 망각 게이트까지 직접적으로 연결된다. 또한 셀 출력 유닛들은 네트워크의 출력층에 연결돼있다. 편향을 무시하는 각 메모리 블록에 있는 하나의 셀이 있는 표준 LSTM 네트워크에 있는 파라미터의 총 개수 N은 $N = n_{c} * n_{c} * 4 + n_{i} * n_{c} * 4 + n_{c} * n_{o} + n_{c} * 3$ 으로 계산된다. $n_{c}$는 메모리셀의 개수(이번 경우에는 메모리 블록의 개수), $n_{i}$는 입력 유닛의 개수, $n_{o}$는 출력 유닛의 개수이다. SGD 최적화로 가중치, 시점별로 LSTM 모델의 학습의 계산 복잡도는 O(1)이다. 따라서 시점당 학습 계산복잡도는 O(N)이다. 적당한 수의 입력을 이용한 네트워크의 학습시간은 $n_{c} * (4 * n_{c} + n_{o})$ 요인에 의해 좌우된다. 시간적 맥락 정보를 저장하기 위해 많은 개수의 출력 유닛과 많은 개수의 메모리 셀을 요구하는 작업의 경우 LSTM 모델을 학습하는 것은 계산이 매우 많아지게 된다.

  우리는 표준 아키텍처에 대한 대안으로  LSTM 학습의 계산 복잡도를 해결하기 위해 Long Short Term Memory Projected(LSTMP)을 제안한다. 그림 1에 있는 이 아키텍처는 LSTM층 뒤에 분리된 선형 투사층을 가지고 있다. 순환 연결은 이 순환 투사층부터 LSTM층의 입력까지 연결된다. 네트워크 출력 유닛은 이 순환 층에 연결돼있다. 이 모델의 파라미터의 수는 $n_c * n_r * 4 + n_i * n_{c} * 4 + n_{r} * n_{o} + n_{c} * n_{r} + n_{c} * 3$이다. $n_{r}$은 순환 투사층에 잇는 유닛의 개수이다. 이 경우 모델의 크기와 학습 계산복잡도는 $n_{r} * (4 * n_{c} + n_{o})$ 요인에 의해 영향을 받는다. 따라서 우리는 $n_r \over n_c$배 만큼 매개변수의 개수를 줄일 수 있다. $n_r < n_c$로 세팅함으로써 우리는 모델 메모라 $n_c$를 늘릴 수 있고 순환 연결과 출력층에 있는 매개변수의 수를 통제할 수 있다.

  제안된 LSTMP 아키텍처로 네트워크 유닛들의 활성화를 위한 방정식이 약간 바뀌었고 활성화 벡터 $m_{t-1}$ 는 $r_{t-1}$로 대체되었고 다음 수식이 추가되었다.

![rnn 논문리뷰 수식1](/images/2023-12-04-LSTM/rnn 논문리뷰 수식1.jpg)

r은 순환 유닛 활성화값을 의미한다.



### 2.4 Deep LSTMP

deep LSTM과 비슷하게 우리는 별도의 순환 투사층이 쌓여있는 여러 LSTM층이 있는 deep LSTMP를 제안한다. LSTMP는 모델의 메모리가 출력층과 순환 연결로부터 독립적으로 늘릴 수 있게 한다. 하지만 메모리 크기를 늘리는것은 모델이 입력 시퀀스 데이터를 기억하여 오버피팅에 취약하게 만드는 것을 기억해야 한다. 우리는 깊이를 늘려서 DNN이 확인하지 않은 예제에 대해 더 나은 일반화를 할 수 있는 것을 알고있다. 깊이는 훈련 데이터에 대해 과적합하기 힘들게 만든다. 왜냐하면 네트워크의 입력은 많은 비선형 함수를 지나야 하기 때문이다. 이런 동기로 메모리 크기와 모델의 일반화 능력을 기르는 것을 목표로 하는 deep LSTMP 아키텍처를 실험했다.



## 3. Distributed Training : Scaling up to Large Models with Parallelization

우리는 LSTM RNN 아키텍처를 구현하기 위해 GPU보다는 멀티코어 CPU를 선택했다. 이 선택은 CPU의 상대적으로 단순한 구현 복잡도, 디버깅의 용이함 그리고 상용 하드웨어로 만든 클러스터를 사용하는 능력을 기반으로 했다. 행렬연산에서 우리는 Eigen 행렬 라이브러리를 사용했다. 이 템플릿 기반 C++ 라이브러리는 벡터화된 명령어를 사용하는 CPU에서의 행렬 연산을 효율적으로 구현한다. 우리의 구현된 활성화 함수와 행렬을 이용한 기울기 계산은 병렬화에 이점을 얻기 위해 SIMD 명령을 사용한다.

  우리는 truncated BPTT 학습 알고리즘을 사용하여 훈련 발화의 짧은 서브시퀀스에 대한 매개변수의 기울기를 계산한다. 활성화값은 고정된 시점 $T_bptt$동안 순방향으로 전파된다.